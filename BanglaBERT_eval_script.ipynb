{"cells":[{"cell_type":"markdown","metadata":{"id":"Noik9q9c7Bhm"},"source":["# [Sentiment Analysis Shared Task](https://github.com/blp-workshop/blp_task2) at [BLP Workshop](https://blp-workshop.github.io/) @EMNLP 2023\n","\n","The main objective of this task is to detect the sentiment associated within a given text. This is a multi-class classification task that involves determining whether the sentiment expressed in the text is Positive, Negative, Neutral.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KSxBhCps7oBf"},"source":["### Downloading dataset from github"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BvwQNYHk6kV5"},"outputs":[],"source":["# !wget https://raw.githubusercontent.com/blp-workshop/blp_task2/main/data/blp23_sentiment_train.tsv\n","# !wget https://raw.githubusercontent.com/blp-workshop/blp_task2/main/data/blp23_sentiment_dev.tsv\n","# !wget https://raw.githubusercontent.com/blp-workshop/blp_task2/main/data/blp23_sentiment_dev_test.tsv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bMzfE34iHyGV"},"outputs":[],"source":["train_file = 'data/blp23_sentiment_train.tsv'\n","validation_file = 'data/blp23_sentiment_dev.tsv'\n","test_file = 'data/blp23_sentiment_test.tsv'\n","test_file_with_label = 'data/blp23_sentiment_test_with_label.tsv'\n","\n","# test_file = 'data/blp23_sentiment_dev_test.tsv'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":687,"status":"ok","timestamp":1693856414303,"user":{"displayName":"Majidur Rahman","userId":"05456759510327186489"},"user_tz":240},"id":"z9W1gZ507Kpw","outputId":"89c9a1ed-728e-405b-d47d-2d9183074de9"},"outputs":[{"name":"stdout","output_type":"stream","text":["len(train_stat_pd): 35266\n","len(validation_stat_pd): 3934\n","len(test_stat_pd): 6707\n"]}],"source":["import pandas as pd\n","train_stat_pd = pd.read_csv(train_file, sep='\\t')\n","validation_stat_pd = pd.read_csv(validation_file, sep='\\t')\n","test_stat_pd = pd.read_csv(test_file_with_label, sep='\\t')\n","\n","print(\"len(train_stat_pd):\", len(train_stat_pd))\n","print(\"len(validation_stat_pd):\", len(validation_stat_pd))\n","print(\"len(test_stat_pd):\", len(test_stat_pd))"]},{"cell_type":"markdown","metadata":{"id":"xYZ96DWt-TZk"},"source":["### installing required libraries.\n"," - transformers\n"," - datasets\n"," - evaluate\n"," - accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17907,"status":"ok","timestamp":1693856435045,"user":{"displayName":"Majidur Rahman","userId":"05456759510327186489"},"user_tz":240},"id":"SLJh5GGU-xET","outputId":"6a8f75f2-0f48-4ba5-af85-c3e438ebc821"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.14.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.16.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.22.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}],"source":["!pip install transformers\n","!pip install datasets\n","!pip install evaluate\n","!pip install --upgrade accelerate"]},{"cell_type":"markdown","metadata":{"id":"OXhVWUJ3A_hx"},"source":["#### importing required libraries and setting up logger"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VIUAU0rRBOmR"},"outputs":[],"source":["import logging\n","import os\n","import random\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","import pandas as pd\n","import datasets\n","import evaluate\n","import numpy as np\n","from datasets import load_dataset, Dataset, DatasetDict\n","import torch\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    PretrainedConfig,\n","    Trainer,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version, send_example_telemetry\n","from transformers.utils.versions import require_version\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","logging.basicConfig(\n","    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","    datefmt=\"%m/%d/%Y %H:%M:%S\",\n","    handlers=[logging.StreamHandler(sys.stdout)],\n",")"]},{"cell_type":"markdown","metadata":{"id":"HP6CdL7NHpxJ"},"source":["### Defining the training, validation, and test data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":538,"status":"ok","timestamp":1693856441394,"user":{"displayName":"Majidur Rahman","userId":"05456759510327186489"},"user_tz":240},"id":"4rxUiGofP8Dy","outputId":"140f1d65-d744-4a38-acb6-9fb59e43845b"},"outputs":[{"name":"stdout","output_type":"stream","text":["# Positives: 12364\n","# Neutrals: 7135\n","# Negatives: 15767\n"]}],"source":["train_data_stat = pd.read_csv(train_file, sep='\\t')\n","\n","positive_data = len(train_data_stat[train_data_stat['label'] == 'Positive'])\n","neutral_data = len(train_data_stat[train_data_stat['label'] == 'Neutral'])\n","negative_data = len(train_data_stat[train_data_stat['label'] == 'Negative'])\n","\n","print(\"# Positives:\", positive_data)\n","print(\"# Neutrals:\", neutral_data)\n","print(\"# Negatives:\", negative_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ShDQq20cPvcT"},"outputs":[],"source":["import torch\n","from torch import nn"]},{"cell_type":"markdown","metadata":{"id":"3-_w4YehCgX4"},"source":["### Setting up the training parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-GUUNj0BPbu"},"outputs":[],"source":["training_args = TrainingArguments(\n","    learning_rate=3-05,\n","    num_train_epochs=10,\n","    weight_decay=0.001,\n","    lr_scheduler_type='linear',\n","    warmup_ratio=0.001,\n","    seed=18,\n","    do_train=True,\n","    do_eval=True,\n","    do_predict=True,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=64,\n","    gradient_accumulation_steps=8,\n","    gradient_checkpointing=True,\n","    fp16=True,\n","    output_dir=\"./BanglaBERT_large/\",\n","    evaluation_strategy=\"epoch\",\n","    metric_for_best_model=\"accuracy\",\n","    overwrite_output_dir=True,\n","    remove_unused_columns=True,\n","    local_rank= 1,\n","    load_best_model_at_end=True,\n","    save_total_limit=2,\n","    save_strategy=\"epoch\"\n",")\n","\n","max_train_samples = None\n","max_eval_samples=None\n","max_predict_samples=None\n","max_seq_length = 512\n","batch_size = 512"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":580,"status":"ok","timestamp":1693856450502,"user":{"displayName":"Majidur Rahman","userId":"05456759510327186489"},"user_tz":240},"id":"0Q4deAnUJ0iI","outputId":"05b2ec4a-8e5b-4a96-9f76-8153befbe5aa"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: True\n","INFO:__main__:Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=True,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=8,\n","gradient_checkpointing=True,\n","greater_is_better=True,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=3.058092225516476e-05,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=./BanglaBERT_large/runs/Sep04_19-40-47_d97a954d817e,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=accuracy,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=10,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=./BanglaBERT_large/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=64,\n","per_device_train_batch_size=32,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=./BanglaBERT_large/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=2,\n","seed=18,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.001,\n","warmup_steps=0,\n","weight_decay=0.001,\n",")\n"]}],"source":["transformers.utils.logging.set_verbosity_info()\n","\n","log_level = training_args.get_process_log_level()\n","logger.setLevel(log_level)\n","datasets.utils.logging.set_verbosity(log_level)\n","transformers.utils.logging.set_verbosity(log_level)\n","transformers.utils.logging.enable_default_handler()\n","transformers.utils.logging.enable_explicit_format()\n","logger.warning(\n","    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","    + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",")\n","logger.info(f\"Training/evaluation parameters {training_args}\")"]},{"cell_type":"markdown","metadata":{"id":"RgkvwlbFHVo5"},"source":["#### Defining the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-De1tz5qHYre"},"outputs":[],"source":["model_name = 'csebuetnlp/banglabert_large'"]},{"cell_type":"markdown","metadata":{"id":"yPqrrDbcKN8n"},"source":["#### setting the random seed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvKpoxaQKTB6"},"outputs":[],"source":["set_seed(training_args.seed)"]},{"cell_type":"markdown","metadata":{"id":"bgNrs7AhKdvl"},"source":["#### Loading data files"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2092,"status":"ok","timestamp":1693856455170,"user":{"displayName":"Majidur Rahman","userId":"05456759510327186489"},"user_tz":240},"id":"LDwaW8AnKcgD","outputId":"03cfde03-31f9-46e6-f764-fcbb74f68e1e"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:__main__:loading a local file for train\n","INFO:__main__:loading a local file for validation\n","INFO:__main__:loading a local file for test\n"]}],"source":["l2id = {'Positive': 2, 'Neutral': 1, 'Negative': 0}\n","train_df = pd.read_csv(train_file, sep='\\t')\n","train_df['label'] = train_df['label'].map(l2id)\n","train_df = Dataset.from_pandas(train_df)\n","validation_df = pd.read_csv(validation_file, sep='\\t')\n","validation_df['label'] = validation_df['label'].map(l2id)\n","validation_df = Dataset.from_pandas(validation_df)\n","test_df = pd.read_csv(test_file, sep='\\t')\n","#test_df['label'] = test_df['label'].map(l2id)\n","test_df = Dataset.from_pandas(test_df)\n","\n","data_files = {\"train\": train_df, \"validation\": validation_df, \"test\": test_df}\n","for key in data_files.keys():\n","    logger.info(f\"loading a local file for {key}\")\n","raw_datasets = DatasetDict(\n","    {\"train\": train_df, \"validation\": validation_df, \"test\": test_df}\n",")"]},{"cell_type":"markdown","metadata":{"id":"BJhNu7tPQ2RU"},"source":["##### Extracting number of unique labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTl6NNPmOXhO"},"outputs":[],"source":["# Labels\n","label_list = raw_datasets[\"train\"].unique(\"label\")\n","label_list.sort()  # sort the labels for determine\n","num_labels = len(label_list)"]},{"cell_type":"markdown","metadata":{"id":"J1dpoOAPRJnN"},"source":["### Loading Pretrained Configuration, Tokenizer and Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5272,"status":"ok","timestamp":1693856462086,"user":{"displayName":"Majidur Rahman","userId":"05456759510327186489"},"user_tz":240},"id":"jmAaMuBuRQd2","outputId":"b5ba2691-a2a6-49d4-9dae-3e05c66e5d4a"},"outputs":[{"name":"stderr","output_type":"stream","text":["[INFO|configuration_utils.py:715] 2023-09-04 19:40:56,114 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert_large/snapshots/a64fb146d81ec4d7f8838b85e084b0c6a325a22f/config.json\n","[INFO|configuration_utils.py:775] 2023-09-04 19:40:56,123 >> Model config ElectraConfig {\n","  \"_name_or_path\": \"csebuetnlp/banglabert_large\",\n","  \"_num_labels\": 2,\n","  \"amp\": true,\n","  \"architectures\": [\n","    \"ElectraForPreTraining\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"embedding_size\": 1024,\n","  \"finetuning_task\": \"text-classification\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"electra\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"summary_activation\": \"gelu\",\n","  \"summary_last_dropout\": 0.1,\n","  \"summary_type\": \"first\",\n","  \"summary_use_proj\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.32.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","[INFO|configuration_utils.py:715] 2023-09-04 19:40:56,376 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert_large/snapshots/a64fb146d81ec4d7f8838b85e084b0c6a325a22f/config.json\n","[INFO|configuration_utils.py:775] 2023-09-04 19:40:56,378 >> Model config ElectraConfig {\n","  \"_name_or_path\": \"csebuetnlp/banglabert_large\",\n","  \"_num_labels\": 2,\n","  \"amp\": true,\n","  \"architectures\": [\n","    \"ElectraForPreTraining\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"embedding_size\": 1024,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"electra\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"summary_activation\": \"gelu\",\n","  \"summary_last_dropout\": 0.1,\n","  \"summary_type\": \"first\",\n","  \"summary_use_proj\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.32.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","[INFO|tokenization_utils_base.py:1852] 2023-09-04 19:40:56,382 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert_large/snapshots/a64fb146d81ec4d7f8838b85e084b0c6a325a22f/vocab.txt\n","[INFO|tokenization_utils_base.py:1852] 2023-09-04 19:40:56,382 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1852] 2023-09-04 19:40:56,383 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1852] 2023-09-04 19:40:56,384 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert_large/snapshots/a64fb146d81ec4d7f8838b85e084b0c6a325a22f/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:1852] 2023-09-04 19:40:56,385 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert_large/snapshots/a64fb146d81ec4d7f8838b85e084b0c6a325a22f/tokenizer_config.json\n","[INFO|configuration_utils.py:715] 2023-09-04 19:40:56,388 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert_large/snapshots/a64fb146d81ec4d7f8838b85e084b0c6a325a22f/config.json\n","[INFO|configuration_utils.py:775] 2023-09-04 19:40:56,391 >> Model config ElectraConfig {\n","  \"_name_or_path\": \"csebuetnlp/banglabert_large\",\n","  \"_num_labels\": 2,\n","  \"amp\": true,\n","  \"architectures\": [\n","    \"ElectraForPreTraining\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"embedding_size\": 1024,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"electra\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"summary_activation\": \"gelu\",\n","  \"summary_last_dropout\": 0.1,\n","  \"summary_type\": \"first\",\n","  \"summary_use_proj\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.32.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","[INFO|configuration_utils.py:715] 2023-09-04 19:40:56,435 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert_large/snapshots/a64fb146d81ec4d7f8838b85e084b0c6a325a22f/config.json\n","[INFO|configuration_utils.py:775] 2023-09-04 19:40:56,437 >> Model config ElectraConfig {\n","  \"_name_or_path\": \"csebuetnlp/banglabert_large\",\n","  \"_num_labels\": 2,\n","  \"amp\": true,\n","  \"architectures\": [\n","    \"ElectraForPreTraining\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"embedding_size\": 1024,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"electra\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"summary_activation\": \"gelu\",\n","  \"summary_last_dropout\": 0.1,\n","  \"summary_type\": \"first\",\n","  \"summary_use_proj\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.32.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","[INFO|modeling_utils.py:2779] 2023-09-04 19:40:56,492 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert_large/snapshots/a64fb146d81ec4d7f8838b85e084b0c6a325a22f/pytorch_model.bin\n","[INFO|modeling_utils.py:3541] 2023-09-04 19:41:00,114 >> Some weights of the model checkpoint at csebuetnlp/banglabert_large were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n","- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3553] 2023-09-04 19:41:00,116 >> Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert_large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["config = AutoConfig.from_pretrained(\n","    model_name,\n","    num_labels=num_labels,\n","    finetuning_task=\"text-classification\",\n","    cache_dir=None,\n","    revision=\"main\",\n","    use_auth_token=None,\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    model_name,\n","    cache_dir=None,\n","    use_fast=True,\n","    revision=\"main\",\n","    use_auth_token=None,\n",")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    from_tf=bool(\".ckpt\" in model_name),\n","    config=config,\n","    cache_dir=None,\n","    revision=\"main\",\n","    use_auth_token=None,\n","    ignore_mismatched_sizes=False,\n",")"]},{"cell_type":"markdown","metadata":{"id":"m7PIQVypeTf4"},"source":["#### Preprocessing the raw_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["7f8cf6bc7e3d4abb9331aebc3a62a1c3","b799fb6a3289419582380969bc55139d","cefb106bb3e645a99091d34b58144089","8b09c22b2af04e2bb2f8922ef9e9db44","3acda243496143f68a43fcde22ee5961","a12536eb44064a209f262a3b66953d54","3280ea42e690437499182b429c8de08b","8ba9cc96be26492097ebe21c8b51d025","0c377590591847aca485295ba95442c2","e7025ed72d964afdb60347a541994915","08c23ff1c1af41a7b7c888b9ad17cec3","5351971c9e5343acb8af0579f211baf7","e2156c08181541f2a8597e0d7fd6cdb3","74fac5f69149449fa765baf661e27810","244c6a59959a441a8d40359a4f5f2ef0","f0c1c79144354fdcb7394c2eac0cd7a7","5a4db8ef98bc4d029aea3290a6eab97b","28d1566d9284405987b1df6d7ffedac7","78618ef49d5b413dac37bd0958874bec","1d3a16909ca44035bfbda821c9b2f073","701902b8e7ee4ebab2292a1927a8aa25","43b795c39ba841d58b746768f638fb7a","13bf7b87dcc14ff88d371c5cb45ec690","55f56abd62824d3598b8c3a030c8610b","914712bf46454f5fbcadded808f7d431","d5c29c669e544454ad5942d02d6362fa","0544bc1dbddc4e5997fbdde3814f2596","a9c808b4fa3b4b77b89080a781857051","17b4389a56504aa8835abc603cffb655","582c9bb03d6c463d9e97b21b95f33c65","66c3ac8a5e6042f5bea1043f18a9154b","e062ae2642bb40c3b22da1cd1a9bed44","92302051439d4d02a148dc25376dd0e3"]},"executionInfo":{"elapsed":7979,"status":"ok","timestamp":1693856470048,"user":{"displayName":"Majidur Rahman","userId":"05456759510327186489"},"user_tz":240},"id":"pqO3YWAZelhd","outputId":"57ab0042-d404-4bd1-949d-14d6bdb1ad0d"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f8cf6bc7e3d4abb9331aebc3a62a1c3","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on dataset:   0%|          | 0/35266 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5351971c9e5343acb8af0579f211baf7","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on dataset:   0%|          | 0/3934 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13bf7b87dcc14ff88d371c5cb45ec690","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on dataset:   0%|          | 0/6707 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n","sentence1_key= non_label_column_names[1]\n","\n","# Padding strategy\n","padding = \"max_length\"\n","\n","# Some models have set the order of the labels to use, so let's make sure we do use it.\n","label_to_id = None\n","if (model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id):\n","    # Some have all caps in their config, some don't.\n","    label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n","    if sorted(label_name_to_id.keys()) == sorted(label_list):\n","        label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n","    else:\n","        logger.warning(\n","            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n","            f\"model labels: {sorted(label_name_to_id.keys())}, dataset labels: {sorted(label_list)}.\"\n","            \"\\nIgnoring the model labels as a result.\",)\n","\n","if label_to_id is not None:\n","    model.config.label2id = label_to_id\n","    model.config.id2label = {id: label for label, id in config.label2id.items()}\n","\n","if 512 > tokenizer.model_max_length:\n","    logger.warning(\n","        f\"The max_seq_length passed ({512}) is larger than the maximum length for the\"\n","        f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\")\n","max_seq_length = min(512, tokenizer.model_max_length)\n","\n","def preprocess_function(examples):\n","    # Tokenize the texts\n","    args = (\n","        (examples[sentence1_key],))\n","    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n","\n","    # Map labels to IDs (not necessary for GLUE tasks)\n","    if label_to_id is not None and \"label\" in examples:\n","        result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n","    return result\n","raw_datasets = raw_datasets.map(\n","    preprocess_function,\n","    batched=True,\n","    load_from_cache_file=True,\n","    desc=\"Running tokenizer on dataset\",\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"ASxWKiqifb_g"},"source":["#### Finalize the training data for training the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QHoDqrBGgD6F"},"outputs":[],"source":["if \"train\" not in raw_datasets:\n","    raise ValueError(\"requires a train dataset\")\n","train_dataset = raw_datasets[\"train\"]\n","if max_train_samples is not None:\n","    max_train_samples_n = min(len(train_dataset), max_train_samples)\n","    train_dataset = train_dataset.select(range(max_train_samples_n))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1693856470049,"user":{"displayName":"Majidur Rahman","userId":"05456759510327186489"},"user_tz":240},"id":"FqME25nm-hwo","outputId":"19a9c4f6-06b1-4058-da76-6092d20a4340"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['id', 'text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 35266\n","})"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset"]},{"cell_type":"markdown","metadata":{"id":"k72vUTSigOzZ"},"source":["#### Finalize the development/evaluation data for evaluating the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MqrW8ospgUYZ"},"outputs":[],"source":["if \"validation\" not in raw_datasets:\n","    raise ValueError(\"requires a validation dataset\")\n","eval_dataset = raw_datasets[\"validation\"]\n","if max_eval_samples is not None:\n","    max_eval_samples_n = min(len(eval_dataset), max_eval_samples)\n","    eval_dataset = eval_dataset.select(range(max_eval_samples_n))"]},{"cell_type":"markdown","metadata":{"id":"B7sVqp3hgU4i"},"source":["#### Finalize the test data for predicting the unseen test data using the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0dBjIQggcYs"},"outputs":[],"source":["if \"test\" not in raw_datasets and \"test_matched\" not in raw_datasets:\n","    raise ValueError(\"requires a test dataset\")\n","predict_dataset = raw_datasets[\"test\"]\n","if max_predict_samples is not None:\n","    max_predict_samples_n = min(len(predict_dataset), max_predict_samples)\n","    predict_dataset = predict_dataset.select(range(max_predict_samples_n))"]},{"cell_type":"markdown","metadata":{"id":"Cqbo1xzRge36"},"source":["#### Log a few random samples from the training set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1693856470049,"user":{"displayName":"Majidur Rahman","userId":"05456759510327186489"},"user_tz":240},"id":"wIO2bxSVgkLb","outputId":"502909c2-2201-453d-a747-375a886f057d"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:__main__:Sample 11879 of the training set: {'id': 'sentinob_11474', 'text': '৩০ টাকায় এতো কিছু । মাশাআল্লাহ', 'label': 2, 'input_ids': [2, 2415, 1, 2564, 916, 205, 6908, 415, 19201, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","INFO:__main__:Sample 8048 of the training set: {'id': '4909', 'text': 'তোমার বাবারা যে প্রতিদিন মুসলমান কে হত্যা করছে বিশ্বের আনাছেকানাছে ইরাক সিরি্য়া লিবিয়া ফিলিস্তীন ইয়ামান চেচনিয়া বসনিয়া কাশ্মির আর্কান ইন্ডিয়া বাংলাদেশ পাকিস্তান মিশর চিন সহ অসংখ্য দেশেআর ২০ টি বছর যাবত যে নির্বিচারে আফগানে হামলা চালিয়ে হাফেজে কুরআন শিশু বাচ্চা - বৃদ্ধা মা বোন কেউ তোমাদের অসভ্যা নিকৃষ্ট জাতি থেকে রক্ষা পায়নি এ অমানুষ নামের জানোয়ারদের হাতে লাখ লাখ মুসলিম হত্যা হয়েছে , কুরআন শিক্ষার মাদ্রাসা সহ ধ্বংস করেছে হাজার হাজার গ্রাম খবর আছে ওহে বেঈমানের দল ', 'label': 0, 'input_ids': [2, 1188, 30263, 831, 3117, 3518, 877, 2562, 1409, 3097, 3331, 774, 6831, 415, 774, 13399, 1, 1, 10517, 1064, 1, 995, 5420, 951, 1, 6359, 784, 17894, 767, 1, 1086, 2434, 13541, 3346, 1118, 5364, 1772, 3173, 1151, 1594, 1248, 7040, 831, 21466, 6262, 410, 3794, 1, 19215, 410, 9867, 2302, 3867, 17, 13709, 855, 2668, 1206, 2766, 15357, 415, 22049, 4676, 842, 3666, 6532, 217, 16577, 2653, 1, 1406, 2261, 2261, 2886, 2562, 1, 16, 9867, 6018, 9891, 1118, 4153, 1333, 1440, 1440, 1508, 1691, 972, 24685, 985, 502, 5006, 1382, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","INFO:__main__:Sample 29414 of the training set: {'id': '719', 'text': 'ঐ ব্রো পড়ালেখা তো কিছুটা হয়লেও করেছো বলে মনে হয়তেছে না আর না হলে ২০২১ এ আসে বলে ফিলিস্তিন নাকি ইহুদিদের ভিটা আগে ইতিহাস পড় তারপর কমেন্ট করতে আইসো অনর্থক নিজের মূর্খতার পরিচয় দেওয়ার দরকার নাই তো', 'label': 0, 'input_ids': [2, 218, 16518, 1, 846, 2623, 1, 12801, 896, 996, 1, 795, 815, 795, 1163, 20808, 217, 1591, 896, 23237, 1708, 30539, 13344, 415, 1090, 3093, 1, 1422, 8764, 924, 5261, 413, 947, 20841, 1204, 17160, 1167, 1, 1, 2119, 1538, 846, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n"]}],"source":["for index in random.sample(range(len(train_dataset)), 3):\n","    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")"]},{"cell_type":"markdown","metadata":{"id":"nAcn0Pc8gogF"},"source":["#### Get the metric function `accuracy`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMWMQdaUgvAq"},"outputs":[],"source":["metric = evaluate.load(\"accuracy\")"]},{"cell_type":"markdown","metadata":{"id":"foWUyuBHgxbA"},"source":["#### Predictions and label_ids field and has to return a dictionary string to float."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-3VqxkqcgxCC"},"outputs":[],"source":["def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.argmax(preds, axis=1)\n","    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"]},{"cell_type":"markdown","metadata":{"id":"dNWK1Hfbg8-o"},"source":["#### Data Collator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_w6lNh-OhJLC"},"outputs":[],"source":["# data_collator = default_data_collator\n","data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)"]},{"cell_type":"markdown","metadata":{"id":"2nYlugPRhNbg"},"source":["#### Initialize our Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeJco0JOhPHx"},"outputs":[],"source":["\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"markdown","metadata":{"id":"cUxWn9HrhqRM"},"source":["#### Training our model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":8028129,"status":"ok","timestamp":1693864501789,"user":{"displayName":"Majidur Rahman","userId":"05456759510327186489"},"user_tz":240},"id":"B681qnPFhtY0","outputId":"1791d12d-bbd3-494d-9907-559e086f1868"},"outputs":[{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:750] 2023-09-04 19:41:13,430 >> The following columns in the training set don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:1714] 2023-09-04 19:41:13,450 >> ***** Running training *****\n","[INFO|trainer.py:1715] 2023-09-04 19:41:13,451 >>   Num examples = 35,266\n","[INFO|trainer.py:1716] 2023-09-04 19:41:13,452 >>   Num Epochs = 10\n","[INFO|trainer.py:1717] 2023-09-04 19:41:13,453 >>   Instantaneous batch size per device = 32\n","[INFO|trainer.py:1720] 2023-09-04 19:41:13,453 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n","[INFO|trainer.py:1721] 2023-09-04 19:41:13,455 >>   Gradient Accumulation steps = 8\n","[INFO|trainer.py:1722] 2023-09-04 19:41:13,455 >>   Total optimization steps = 1,370\n","[INFO|trainer.py:1723] 2023-09-04 19:41:13,458 >>   Number of trainable parameters = 336,658,435\n","[WARNING|logging.py:290] 2023-09-04 19:41:13,500 >> You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1370' max='1370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1370/1370 2:13:41, Epoch 9/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>No log</td>\n","      <td>0.729222</td>\n","      <td>0.710219</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.694453</td>\n","      <td>0.726233</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.788647</td>\n","      <td>0.713523</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.575800</td>\n","      <td>0.913463</td>\n","      <td>0.706660</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.575800</td>\n","      <td>1.032114</td>\n","      <td>0.710727</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.575800</td>\n","      <td>1.202490</td>\n","      <td>0.703864</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.575800</td>\n","      <td>1.226838</td>\n","      <td>0.704881</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.184200</td>\n","      <td>1.283895</td>\n","      <td>0.711998</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.184200</td>\n","      <td>1.359632</td>\n","      <td>0.706914</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.184200</td>\n","      <td>1.396003</td>\n","      <td>0.709456</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:750] 2023-09-04 19:54:05,400 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3119] 2023-09-04 19:54:05,404 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3121] 2023-09-04 19:54:05,405 >>   Num examples = 3934\n","[INFO|trainer.py:3124] 2023-09-04 19:54:05,406 >>   Batch size = 64\n","[INFO|trainer.py:2845] 2023-09-04 19:54:27,922 >> Saving model checkpoint to ./BanglaBERT_large/checkpoint-137\n","[INFO|configuration_utils.py:460] 2023-09-04 19:54:27,927 >> Configuration saved in ./BanglaBERT_large/checkpoint-137/config.json\n","[INFO|modeling_utils.py:1953] 2023-09-04 19:54:30,884 >> Model weights saved in ./BanglaBERT_large/checkpoint-137/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2235] 2023-09-04 19:54:34,711 >> tokenizer config file saved in ./BanglaBERT_large/checkpoint-137/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2242] 2023-09-04 19:54:34,721 >> Special tokens file saved in ./BanglaBERT_large/checkpoint-137/special_tokens_map.json\n","[INFO|trainer.py:750] 2023-09-04 20:07:32,110 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3119] 2023-09-04 20:07:32,114 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3121] 2023-09-04 20:07:32,115 >>   Num examples = 3934\n","[INFO|trainer.py:3124] 2023-09-04 20:07:32,116 >>   Batch size = 64\n","[INFO|trainer.py:2845] 2023-09-04 20:07:54,668 >> Saving model checkpoint to ./BanglaBERT_large/checkpoint-275\n","[INFO|configuration_utils.py:460] 2023-09-04 20:07:54,673 >> Configuration saved in ./BanglaBERT_large/checkpoint-275/config.json\n","[INFO|modeling_utils.py:1953] 2023-09-04 20:07:57,664 >> Model weights saved in ./BanglaBERT_large/checkpoint-275/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2235] 2023-09-04 20:07:57,671 >> tokenizer config file saved in ./BanglaBERT_large/checkpoint-275/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2242] 2023-09-04 20:08:01,317 >> Special tokens file saved in ./BanglaBERT_large/checkpoint-275/special_tokens_map.json\n","[INFO|trainer.py:750] 2023-09-04 20:20:58,635 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3119] 2023-09-04 20:20:58,638 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3121] 2023-09-04 20:20:58,640 >>   Num examples = 3934\n","[INFO|trainer.py:3124] 2023-09-04 20:20:58,641 >>   Batch size = 64\n","[INFO|trainer.py:2845] 2023-09-04 20:21:21,173 >> Saving model checkpoint to ./BanglaBERT_large/checkpoint-413\n","[INFO|configuration_utils.py:460] 2023-09-04 20:21:21,179 >> Configuration saved in ./BanglaBERT_large/checkpoint-413/config.json\n","[INFO|modeling_utils.py:1953] 2023-09-04 20:21:24,157 >> Model weights saved in ./BanglaBERT_large/checkpoint-413/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2235] 2023-09-04 20:21:24,164 >> tokenizer config file saved in ./BanglaBERT_large/checkpoint-413/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2242] 2023-09-04 20:21:24,168 >> Special tokens file saved in ./BanglaBERT_large/checkpoint-413/special_tokens_map.json\n","[INFO|trainer.py:2932] 2023-09-04 20:21:34,373 >> Deleting older checkpoint [BanglaBERT_large/checkpoint-137] due to args.save_total_limit\n","[INFO|trainer.py:750] 2023-09-04 20:34:25,269 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3119] 2023-09-04 20:34:25,272 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3121] 2023-09-04 20:34:25,273 >>   Num examples = 3934\n","[INFO|trainer.py:3124] 2023-09-04 20:34:25,274 >>   Batch size = 64\n","[INFO|trainer.py:2845] 2023-09-04 20:34:47,806 >> Saving model checkpoint to ./BanglaBERT_large/checkpoint-551\n","[INFO|configuration_utils.py:460] 2023-09-04 20:34:47,812 >> Configuration saved in ./BanglaBERT_large/checkpoint-551/config.json\n","[INFO|modeling_utils.py:1953] 2023-09-04 20:34:50,799 >> Model weights saved in ./BanglaBERT_large/checkpoint-551/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2235] 2023-09-04 20:34:50,805 >> tokenizer config file saved in ./BanglaBERT_large/checkpoint-551/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2242] 2023-09-04 20:34:50,809 >> Special tokens file saved in ./BanglaBERT_large/checkpoint-551/special_tokens_map.json\n","[INFO|trainer.py:2932] 2023-09-04 20:34:58,426 >> Deleting older checkpoint [BanglaBERT_large/checkpoint-413] due to args.save_total_limit\n","[INFO|trainer.py:750] 2023-09-04 20:47:49,451 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3119] 2023-09-04 20:47:49,455 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3121] 2023-09-04 20:47:49,456 >>   Num examples = 3934\n","[INFO|trainer.py:3124] 2023-09-04 20:47:49,457 >>   Batch size = 64\n","[INFO|trainer.py:2845] 2023-09-04 20:48:11,993 >> Saving model checkpoint to ./BanglaBERT_large/checkpoint-689\n","[INFO|configuration_utils.py:460] 2023-09-04 20:48:11,999 >> Configuration saved in ./BanglaBERT_large/checkpoint-689/config.json\n","[INFO|modeling_utils.py:1953] 2023-09-04 20:48:18,605 >> Model weights saved in ./BanglaBERT_large/checkpoint-689/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2235] 2023-09-04 20:48:18,611 >> tokenizer config file saved in ./BanglaBERT_large/checkpoint-689/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2242] 2023-09-04 20:48:18,615 >> Special tokens file saved in ./BanglaBERT_large/checkpoint-689/special_tokens_map.json\n","[INFO|trainer.py:2932] 2023-09-04 20:48:25,006 >> Deleting older checkpoint [BanglaBERT_large/checkpoint-551] due to args.save_total_limit\n","[INFO|trainer.py:750] 2023-09-04 21:01:16,024 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3119] 2023-09-04 21:01:16,028 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3121] 2023-09-04 21:01:16,028 >>   Num examples = 3934\n","[INFO|trainer.py:3124] 2023-09-04 21:01:16,029 >>   Batch size = 64\n","[INFO|trainer.py:2845] 2023-09-04 21:01:38,559 >> Saving model checkpoint to ./BanglaBERT_large/checkpoint-827\n","[INFO|configuration_utils.py:460] 2023-09-04 21:01:38,564 >> Configuration saved in ./BanglaBERT_large/checkpoint-827/config.json\n","[INFO|modeling_utils.py:1953] 2023-09-04 21:01:45,291 >> Model weights saved in ./BanglaBERT_large/checkpoint-827/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2235] 2023-09-04 21:01:45,300 >> tokenizer config file saved in ./BanglaBERT_large/checkpoint-827/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2242] 2023-09-04 21:01:45,303 >> Special tokens file saved in ./BanglaBERT_large/checkpoint-827/special_tokens_map.json\n","[INFO|trainer.py:2932] 2023-09-04 21:01:51,223 >> Deleting older checkpoint [BanglaBERT_large/checkpoint-689] due to args.save_total_limit\n","[INFO|trainer.py:750] 2023-09-04 21:14:42,460 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3119] 2023-09-04 21:14:42,464 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3121] 2023-09-04 21:14:42,465 >>   Num examples = 3934\n","[INFO|trainer.py:3124] 2023-09-04 21:14:42,466 >>   Batch size = 64\n","[INFO|trainer.py:2845] 2023-09-04 21:15:05,009 >> Saving model checkpoint to ./BanglaBERT_large/checkpoint-965\n","[INFO|configuration_utils.py:460] 2023-09-04 21:15:05,014 >> Configuration saved in ./BanglaBERT_large/checkpoint-965/config.json\n","[INFO|modeling_utils.py:1953] 2023-09-04 21:15:11,499 >> Model weights saved in ./BanglaBERT_large/checkpoint-965/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2235] 2023-09-04 21:15:11,507 >> tokenizer config file saved in ./BanglaBERT_large/checkpoint-965/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2242] 2023-09-04 21:15:11,511 >> Special tokens file saved in ./BanglaBERT_large/checkpoint-965/special_tokens_map.json\n","[INFO|trainer.py:2932] 2023-09-04 21:15:21,059 >> Deleting older checkpoint [BanglaBERT_large/checkpoint-827] due to args.save_total_limit\n","[INFO|trainer.py:750] 2023-09-04 21:28:12,198 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3119] 2023-09-04 21:28:12,202 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3121] 2023-09-04 21:28:12,203 >>   Num examples = 3934\n","[INFO|trainer.py:3124] 2023-09-04 21:28:12,204 >>   Batch size = 64\n","[INFO|trainer.py:2845] 2023-09-04 21:28:34,721 >> Saving model checkpoint to ./BanglaBERT_large/checkpoint-1103\n","[INFO|configuration_utils.py:460] 2023-09-04 21:28:34,726 >> Configuration saved in ./BanglaBERT_large/checkpoint-1103/config.json\n","[INFO|modeling_utils.py:1953] 2023-09-04 21:28:41,226 >> Model weights saved in ./BanglaBERT_large/checkpoint-1103/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2235] 2023-09-04 21:28:41,235 >> tokenizer config file saved in ./BanglaBERT_large/checkpoint-1103/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2242] 2023-09-04 21:28:41,239 >> Special tokens file saved in ./BanglaBERT_large/checkpoint-1103/special_tokens_map.json\n","[INFO|trainer.py:2932] 2023-09-04 21:28:51,484 >> Deleting older checkpoint [BanglaBERT_large/checkpoint-965] due to args.save_total_limit\n","[INFO|trainer.py:750] 2023-09-04 21:41:42,568 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3119] 2023-09-04 21:41:42,572 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3121] 2023-09-04 21:41:42,573 >>   Num examples = 3934\n","[INFO|trainer.py:3124] 2023-09-04 21:41:42,573 >>   Batch size = 64\n","[INFO|trainer.py:2845] 2023-09-04 21:42:05,110 >> Saving model checkpoint to ./BanglaBERT_large/checkpoint-1240\n","[INFO|configuration_utils.py:460] 2023-09-04 21:42:05,116 >> Configuration saved in ./BanglaBERT_large/checkpoint-1240/config.json\n","[INFO|modeling_utils.py:1953] 2023-09-04 21:42:07,964 >> Model weights saved in ./BanglaBERT_large/checkpoint-1240/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2235] 2023-09-04 21:42:11,844 >> tokenizer config file saved in ./BanglaBERT_large/checkpoint-1240/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2242] 2023-09-04 21:42:11,849 >> Special tokens file saved in ./BanglaBERT_large/checkpoint-1240/special_tokens_map.json\n","[INFO|trainer.py:2932] 2023-09-04 21:42:21,768 >> Deleting older checkpoint [BanglaBERT_large/checkpoint-1103] due to args.save_total_limit\n","[INFO|trainer.py:750] 2023-09-04 21:54:24,574 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3119] 2023-09-04 21:54:24,578 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3121] 2023-09-04 21:54:24,579 >>   Num examples = 3934\n","[INFO|trainer.py:3124] 2023-09-04 21:54:24,580 >>   Batch size = 64\n","[INFO|trainer.py:2845] 2023-09-04 21:54:47,087 >> Saving model checkpoint to ./BanglaBERT_large/checkpoint-1370\n","[INFO|configuration_utils.py:460] 2023-09-04 21:54:47,093 >> Configuration saved in ./BanglaBERT_large/checkpoint-1370/config.json\n","[INFO|modeling_utils.py:1953] 2023-09-04 21:54:49,991 >> Model weights saved in ./BanglaBERT_large/checkpoint-1370/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2235] 2023-09-04 21:54:53,817 >> tokenizer config file saved in ./BanglaBERT_large/checkpoint-1370/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2242] 2023-09-04 21:54:53,827 >> Special tokens file saved in ./BanglaBERT_large/checkpoint-1370/special_tokens_map.json\n","[INFO|trainer.py:2932] 2023-09-04 21:54:59,770 >> Deleting older checkpoint [BanglaBERT_large/checkpoint-1240] due to args.save_total_limit\n","[INFO|trainer.py:1962] 2023-09-04 21:55:00,078 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2124] 2023-09-04 21:55:00,079 >> Loading best model from ./BanglaBERT_large/checkpoint-275 (score: 0.7262328267097473).\n"]}],"source":["train_result = trainer.train()\n","metrics = train_result.metrics\n","max_train_samples = (\n","    max_train_samples if max_train_samples is not None else len(train_dataset)\n",")\n","metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))"]},{"cell_type":"markdown","metadata":{"id":"SaaRglkwllSp"},"source":["#### Saving the tokenizer too for easy upload"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6399,"status":"ok","timestamp":1693864508168,"user":{"displayName":"Majidur Rahman","userId":"05456759510327186489"},"user_tz":240},"id":"9UwoMEbAloMx","outputId":"00d11bde-4486-4fe3-c8c7-5e08f1617c87"},"outputs":[{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:2845] 2023-09-04 21:55:01,659 >> Saving model checkpoint to ./BanglaBERT_large/\n","[INFO|configuration_utils.py:460] 2023-09-04 21:55:01,665 >> Configuration saved in ./BanglaBERT_large/config.json\n","[INFO|modeling_utils.py:1953] 2023-09-04 21:55:04,890 >> Model weights saved in ./BanglaBERT_large/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2235] 2023-09-04 21:55:04,897 >> tokenizer config file saved in ./BanglaBERT_large/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2242] 2023-09-04 21:55:04,901 >> Special tokens file saved in ./BanglaBERT_large/special_tokens_map.json\n"]},{"name":"stdout","output_type":"stream","text":["***** train metrics *****\n","  epoch                    =        9.94\n","  total_flos               = 304166646GF\n","  train_loss               =      0.3002\n","  train_runtime            =  2:13:48.18\n","  train_samples            =       35266\n","  train_samples_per_second =      43.928\n","  train_steps_per_second   =       0.171\n"]}],"source":["trainer.save_model()\n","trainer.log_metrics(\"train\", metrics)\n","trainer.save_metrics(\"train\", metrics)\n","trainer.save_state()"]},{"cell_type":"markdown","metadata":{"id":"K9zCKBGEhwb7"},"source":["#### Evaluating our model on validation/development data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"executionInfo":{"elapsed":22874,"status":"ok","timestamp":1693864531025,"user":{"displayName":"Majidur Rahman","userId":"05456759510327186489"},"user_tz":240},"id":"YClw3dXTh17u","outputId":"1252e6ad-c56a-4ceb-c914-a8d47c32650f"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:__main__:*** Evaluate ***\n","[INFO|trainer.py:750] 2023-09-04 21:55:07,847 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3119] 2023-09-04 21:55:07,853 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3121] 2023-09-04 21:55:07,854 >>   Num examples = 3934\n","[INFO|trainer.py:3124] 2023-09-04 21:55:07,856 >>   Batch size = 64\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='62' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [62/62 00:22]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       9.94\n","  eval_accuracy           =     0.7262\n","  eval_loss               =     0.6945\n","  eval_runtime            = 0:00:22.91\n","  eval_samples            =       3934\n","  eval_samples_per_second =    171.669\n","  eval_steps_per_second   =      2.706\n"]}],"source":["logger.info(\"*** Evaluate ***\")\n","\n","metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","max_eval_samples = (\n","    max_eval_samples if max_eval_samples is not None else len(eval_dataset)\n",")\n","metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","trainer.log_metrics(\"eval\", metrics)\n","trainer.save_metrics(\"eval\", metrics)"]},{"cell_type":"markdown","metadata":{"id":"Y3LSdUdPh7uG"},"source":["### Predecting the test data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":146},"executionInfo":{"elapsed":38386,"status":"ok","timestamp":1693864569395,"user":{"displayName":"Majidur Rahman","userId":"05456759510327186489"},"user_tz":240},"id":"gnXhVq6Yh_oS","outputId":"d5cde268-c3c7-4b6c-c75f-ab870b53978f"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:__main__:*** Predict ***\n","[INFO|trainer.py:750] 2023-09-04 21:55:30,797 >> The following columns in the test set don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: text. If text are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3119] 2023-09-04 21:55:30,800 >> ***** Running Prediction *****\n","[INFO|trainer.py:3121] 2023-09-04 21:55:30,801 >>   Num examples = 6707\n","[INFO|trainer.py:3124] 2023-09-04 21:55:30,802 >>   Batch size = 64\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:__main__:***** Predict results *****\n"]}],"source":["id2l = {0:'Negative', 1:'Neutral', 2:'Positive'}\n","logger.info(\"*** Predict ***\")\n","#predict_dataset = predict_dataset.remove_columns(\"label\")\n","ids = predict_dataset['id']\n","predict_dataset = predict_dataset.remove_columns(\"id\")\n","predictions = trainer.predict(predict_dataset, metric_key_prefix=\"predict\").predictions\n","predictions = np.argmax(predictions, axis=1)\n","output_predict_file = os.path.join(training_args.output_dir, f\"predict_results.tsv\")\n","if trainer.is_world_process_zero():\n","    with open(output_predict_file, \"w\") as writer:\n","        logger.info(f\"***** Predict results *****\")\n","        writer.write(\"id\\tlabel\\n\")\n","        for index, item in enumerate(predictions):\n","            item = label_list[item]\n","            item = id2l[item]\n","            writer.write(f\"{ids[index]}\\t{item}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1693864569395,"user":{"displayName":"Majidur Rahman","userId":"05456759510327186489"},"user_tz":240},"id":"8Gqqk_24__47","outputId":"c69f17bc-f98c-457f-9294-607a2bd35476"},"outputs":[{"data":{"text/plain":["7135"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["ids[0]"]},{"cell_type":"markdown","metadata":{"id":"fQgoTTIoiI0X"},"source":["#### Saving the model into card"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1693864569739,"user":{"displayName":"Majidur Rahman","userId":"05456759510327186489"},"user_tz":240},"id":"B1ooJgrViLVj","outputId":"2433cfe9-43fb-4c81-f1b9-6605b1158379"},"outputs":[{"name":"stderr","output_type":"stream","text":["[INFO|modelcard.py:452] 2023-09-04 21:56:09,740 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7262328267097473}]}\n"]}],"source":["kwargs = {\"finetuned_from\": model_name, \"tasks\": \"text-classification\"}\n","trainer.create_model_card(**kwargs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1693864569740,"user":{"displayName":"Majidur Rahman","userId":"05456759510327186489"},"user_tz":240},"id":"puoameKK-de8","outputId":"1f479c25-9ff6-4582-f795-a902c896cdd2"},"outputs":[{"data":{"text/plain":["0.7153719994036082"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import f1_score\n","\n","gold_tsv = pd.read_csv('data/blp23_sentiment_test_with_label.tsv', sep='\\t')\n","prediction_tsv = pd.read_csv(output_predict_file, sep='\\t')\n","\n","f1_score(gold_tsv['label'], prediction_tsv['label'], average='micro')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7RyYAQs2tqmQ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0544bc1dbddc4e5997fbdde3814f2596":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08c23ff1c1af41a7b7c888b9ad17cec3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c377590591847aca485295ba95442c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"13bf7b87dcc14ff88d371c5cb45ec690":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_55f56abd62824d3598b8c3a030c8610b","IPY_MODEL_914712bf46454f5fbcadded808f7d431","IPY_MODEL_d5c29c669e544454ad5942d02d6362fa"],"layout":"IPY_MODEL_0544bc1dbddc4e5997fbdde3814f2596"}},"17b4389a56504aa8835abc603cffb655":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d3a16909ca44035bfbda821c9b2f073":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"244c6a59959a441a8d40359a4f5f2ef0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_701902b8e7ee4ebab2292a1927a8aa25","placeholder":"​","style":"IPY_MODEL_43b795c39ba841d58b746768f638fb7a","value":" 3934/3934 [00:00&lt;00:00, 4889.57 examples/s]"}},"28d1566d9284405987b1df6d7ffedac7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3280ea42e690437499182b429c8de08b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3acda243496143f68a43fcde22ee5961":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43b795c39ba841d58b746768f638fb7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5351971c9e5343acb8af0579f211baf7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e2156c08181541f2a8597e0d7fd6cdb3","IPY_MODEL_74fac5f69149449fa765baf661e27810","IPY_MODEL_244c6a59959a441a8d40359a4f5f2ef0"],"layout":"IPY_MODEL_f0c1c79144354fdcb7394c2eac0cd7a7"}},"55f56abd62824d3598b8c3a030c8610b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9c808b4fa3b4b77b89080a781857051","placeholder":"​","style":"IPY_MODEL_17b4389a56504aa8835abc603cffb655","value":"Running tokenizer on dataset: 100%"}},"582c9bb03d6c463d9e97b21b95f33c65":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a4db8ef98bc4d029aea3290a6eab97b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66c3ac8a5e6042f5bea1043f18a9154b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"701902b8e7ee4ebab2292a1927a8aa25":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74fac5f69149449fa765baf661e27810":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78618ef49d5b413dac37bd0958874bec","max":3934,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1d3a16909ca44035bfbda821c9b2f073","value":3934}},"78618ef49d5b413dac37bd0958874bec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f8cf6bc7e3d4abb9331aebc3a62a1c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b799fb6a3289419582380969bc55139d","IPY_MODEL_cefb106bb3e645a99091d34b58144089","IPY_MODEL_8b09c22b2af04e2bb2f8922ef9e9db44"],"layout":"IPY_MODEL_3acda243496143f68a43fcde22ee5961"}},"8b09c22b2af04e2bb2f8922ef9e9db44":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7025ed72d964afdb60347a541994915","placeholder":"​","style":"IPY_MODEL_08c23ff1c1af41a7b7c888b9ad17cec3","value":" 35266/35266 [00:07&lt;00:00, 5061.83 examples/s]"}},"8ba9cc96be26492097ebe21c8b51d025":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"914712bf46454f5fbcadded808f7d431":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_582c9bb03d6c463d9e97b21b95f33c65","max":6707,"min":0,"orientation":"horizontal","style":"IPY_MODEL_66c3ac8a5e6042f5bea1043f18a9154b","value":6707}},"92302051439d4d02a148dc25376dd0e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a12536eb44064a209f262a3b66953d54":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9c808b4fa3b4b77b89080a781857051":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b799fb6a3289419582380969bc55139d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a12536eb44064a209f262a3b66953d54","placeholder":"​","style":"IPY_MODEL_3280ea42e690437499182b429c8de08b","value":"Running tokenizer on dataset: 100%"}},"cefb106bb3e645a99091d34b58144089":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ba9cc96be26492097ebe21c8b51d025","max":35266,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0c377590591847aca485295ba95442c2","value":35266}},"d5c29c669e544454ad5942d02d6362fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e062ae2642bb40c3b22da1cd1a9bed44","placeholder":"​","style":"IPY_MODEL_92302051439d4d02a148dc25376dd0e3","value":" 6707/6707 [00:01&lt;00:00, 4952.61 examples/s]"}},"e062ae2642bb40c3b22da1cd1a9bed44":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2156c08181541f2a8597e0d7fd6cdb3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a4db8ef98bc4d029aea3290a6eab97b","placeholder":"​","style":"IPY_MODEL_28d1566d9284405987b1df6d7ffedac7","value":"Running tokenizer on dataset: 100%"}},"e7025ed72d964afdb60347a541994915":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0c1c79144354fdcb7394c2eac0cd7a7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}